{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:10:31.767340Z",
     "start_time": "2024-12-06T05:10:28.033658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from blackjack import BlackJackGame\n",
    "import os\n",
    "from TensorboardCallback import TensorboardCallback\n",
    "\n",
    "\n",
    "logdir = \"logs\"\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ],
   "id": "22cb20d21e6f8ab6",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:10:31.778090Z",
     "start_time": "2024-12-06T05:10:31.768346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "blackjack = BlackJackGame(mute=True, num_decks=2)\n",
    "blackjack.reset()"
   ],
   "id": "db6e24af176218cb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "        0, 0, 0, 0]),\n",
       " {})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:10:31.783625Z",
     "start_time": "2024-12-06T05:10:31.779096Z"
    }
   },
   "cell_type": "code",
   "source": "blackjack.action_space.sample()",
   "id": "2fed97cde46b6e7d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:10:31.789825Z",
     "start_time": "2024-12-06T05:10:31.784632Z"
    }
   },
   "cell_type": "code",
   "source": "blackjack.observation_space.sample()",
   "id": "ff63b6989f9991bb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([8, 0, 3, 4, 2, 5, 7, 1, 8, 8, 8, 6, 8, 3, 1, 1, 3, 1, 3, 5, 1, 1,\n",
       "       1, 8, 4, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-06T05:14:46.731659Z",
     "start_time": "2024-12-06T05:13:39.123504Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from stable_baselines3 import A2C\n",
    "from blackjack import BlackJackGame\n",
    "\n",
    "env = BlackJackGame(mute=True, num_decks=1, card_count=False, record_limit=1000)\n",
    "env.reset()\n",
    "model = A2C(\"MlpPolicy\", env, verbose=True, tensorboard_log=logdir, learning_rate=0.001)\n",
    "model.learn(total_timesteps=500000, callback=TensorboardCallback(), tb_log_name=\"A2C_no_memory\")"
   ],
   "id": "adc2fb6643e3f744",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to logs\\A2C_no_memory_7\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.114      |\n",
      "|    loss_rate          | 0.283       |\n",
      "|    win_rate           | 0.169       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1.01        |\n",
      "|    ep_rew_mean        | -0.16       |\n",
      "| time/                 |             |\n",
      "|    fps                | 335         |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 1           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.112      |\n",
      "|    explained_variance | 0.114014626 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | -0.00823    |\n",
      "|    value_loss         | 0.944       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.188      |\n",
      "|    loss_rate          | 0.562       |\n",
      "|    win_rate           | 0.374       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.14       |\n",
      "| time/                 |             |\n",
      "|    fps                | 337         |\n",
      "|    iterations         | 200         |\n",
      "|    time_elapsed       | 2           |\n",
      "|    total_timesteps    | 1000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.0418     |\n",
      "|    explained_variance | -0.41530204 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 199         |\n",
      "|    policy_loss        | 0.00607     |\n",
      "|    value_loss         | 1.55        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.157    |\n",
      "|    loss_rate          | 0.558     |\n",
      "|    win_rate           | 0.401     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.17     |\n",
      "| time/                 |           |\n",
      "|    fps                | 344       |\n",
      "|    iterations         | 300       |\n",
      "|    time_elapsed       | 4         |\n",
      "|    total_timesteps    | 1500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0301   |\n",
      "|    explained_variance | 0.1418938 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 299       |\n",
      "|    policy_loss        | -0.00228  |\n",
      "|    value_loss         | 0.371     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.166    |\n",
      "|    loss_rate          | 0.554     |\n",
      "|    win_rate           | 0.388     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 331       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 6         |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.0176   |\n",
      "|    explained_variance | 0.7372318 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -8.61e-05 |\n",
      "|    value_loss         | 0.252     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.181     |\n",
      "|    loss_rate          | 0.563      |\n",
      "|    win_rate           | 0.382      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.1       |\n",
      "| time/                 |            |\n",
      "|    fps                | 338        |\n",
      "|    iterations         | 500        |\n",
      "|    time_elapsed       | 7          |\n",
      "|    total_timesteps    | 2500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.0172    |\n",
      "|    explained_variance | 0.82073426 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 499        |\n",
      "|    policy_loss        | -0.00157   |\n",
      "|    value_loss         | 0.516      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.226     |\n",
      "|    loss_rate          | 0.59       |\n",
      "|    win_rate           | 0.364      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.37      |\n",
      "| time/                 |            |\n",
      "|    fps                | 341        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 8          |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.0181    |\n",
      "|    explained_variance | 0.52830875 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 0.00179    |\n",
      "|    value_loss         | 0.955      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.22     |\n",
      "|    loss_rate          | 0.586     |\n",
      "|    win_rate           | 0.366     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.27     |\n",
      "| time/                 |           |\n",
      "|    fps                | 346       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 10        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00527  |\n",
      "|    explained_variance | 0.9138938 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -0.000108 |\n",
      "|    value_loss         | 0.0804    |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.164     |\n",
      "|    loss_rate          | 0.557      |\n",
      "|    win_rate           | 0.393      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.29      |\n",
      "| time/                 |            |\n",
      "|    fps                | 347        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 11         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.00168   |\n",
      "|    explained_variance | 0.15309554 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 4.31e-06   |\n",
      "|    value_loss         | 0.543      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.154      |\n",
      "|    loss_rate          | 0.55        |\n",
      "|    win_rate           | 0.396       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.05       |\n",
      "| time/                 |             |\n",
      "|    fps                | 355         |\n",
      "|    iterations         | 900         |\n",
      "|    time_elapsed       | 12          |\n",
      "|    total_timesteps    | 4500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.00164    |\n",
      "|    explained_variance | -0.06871176 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 899         |\n",
      "|    policy_loss        | -9.09e-05   |\n",
      "|    value_loss         | 0.458       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.148     |\n",
      "|    loss_rate          | 0.547      |\n",
      "|    win_rate           | 0.399      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.13      |\n",
      "| time/                 |            |\n",
      "|    fps                | 361        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 13         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.00165   |\n",
      "|    explained_variance | 0.27904844 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | 2.38e-05   |\n",
      "|    value_loss         | 0.711      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.14      |\n",
      "|    loss_rate          | 0.545      |\n",
      "|    win_rate           | 0.405      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.07      |\n",
      "| time/                 |            |\n",
      "|    fps                | 363        |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 15         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.00165   |\n",
      "|    explained_variance | 0.59803617 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -2.85e-05  |\n",
      "|    value_loss         | 0.349      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.18     |\n",
      "|    loss_rate          | 0.567     |\n",
      "|    win_rate           | 0.387     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.32     |\n",
      "| time/                 |           |\n",
      "|    fps                | 366       |\n",
      "|    iterations         | 1200      |\n",
      "|    time_elapsed       | 16        |\n",
      "|    total_timesteps    | 6000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00165  |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1199      |\n",
      "|    policy_loss        | -7.25e-05 |\n",
      "|    value_loss         | 0.311     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.229    |\n",
      "|    loss_rate          | 0.596     |\n",
      "|    win_rate           | 0.367     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.24     |\n",
      "| time/                 |           |\n",
      "|    fps                | 368       |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 17        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.00163  |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | -0.000129 |\n",
      "|    value_loss         | 0.654     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.207      |\n",
      "|    loss_rate          | 0.58        |\n",
      "|    win_rate           | 0.373       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.09       |\n",
      "| time/                 |             |\n",
      "|    fps                | 370         |\n",
      "|    iterations         | 1400        |\n",
      "|    time_elapsed       | 18          |\n",
      "|    total_timesteps    | 7000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000478   |\n",
      "|    explained_variance | 0.061853528 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 1399        |\n",
      "|    policy_loss        | 2.68e-05    |\n",
      "|    value_loss         | 0.984       |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rates/                |              |\n",
      "|    avg_reward         | -0.166       |\n",
      "|    loss_rate          | 0.556        |\n",
      "|    win_rate           | 0.39         |\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 1            |\n",
      "|    ep_rew_mean        | -0.14        |\n",
      "| time/                 |              |\n",
      "|    fps                | 372          |\n",
      "|    iterations         | 1500         |\n",
      "|    time_elapsed       | 20           |\n",
      "|    total_timesteps    | 7500         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -0.000486    |\n",
      "|    explained_variance | -0.029907942 |\n",
      "|    learning_rate      | 0.001        |\n",
      "|    n_updates          | 1499         |\n",
      "|    policy_loss        | 6.05e-06     |\n",
      "|    value_loss         | 1.01         |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.173      |\n",
      "|    loss_rate          | 0.563       |\n",
      "|    win_rate           | 0.39        |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.04       |\n",
      "| time/                 |             |\n",
      "|    fps                | 374         |\n",
      "|    iterations         | 1600        |\n",
      "|    time_elapsed       | 21          |\n",
      "|    total_timesteps    | 8000        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000479   |\n",
      "|    explained_variance | 0.070156515 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 1599        |\n",
      "|    policy_loss        | -3.2e-06    |\n",
      "|    value_loss         | 0.899       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.17     |\n",
      "|    loss_rate          | 0.564     |\n",
      "|    win_rate           | 0.394     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.08     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000479 |\n",
      "|    explained_variance | 0.7747646 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -1.79e-05 |\n",
      "|    value_loss         | 0.387     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.192     |\n",
      "|    loss_rate          | 0.574      |\n",
      "|    win_rate           | 0.382      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.29      |\n",
      "| time/                 |            |\n",
      "|    fps                | 375        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 23         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000484  |\n",
      "|    explained_variance | 0.83571917 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | -2.53e-05  |\n",
      "|    value_loss         | 0.442      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.21      |\n",
      "|    loss_rate          | 0.578      |\n",
      "|    win_rate           | 0.368      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.17      |\n",
      "| time/                 |            |\n",
      "|    fps                | 376        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 25         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000483  |\n",
      "|    explained_variance | 0.24013805 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | 1.13e-05   |\n",
      "|    value_loss         | 0.796      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.241    |\n",
      "|    loss_rate          | 0.594     |\n",
      "|    win_rate           | 0.353     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.33     |\n",
      "| time/                 |           |\n",
      "|    fps                | 375       |\n",
      "|    iterations         | 2000      |\n",
      "|    time_elapsed       | 26        |\n",
      "|    total_timesteps    | 10000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000483 |\n",
      "|    explained_variance | 0.7822082 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 1999      |\n",
      "|    policy_loss        | -1.48e-07 |\n",
      "|    value_loss         | 0.139     |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.263      |\n",
      "|    loss_rate          | 0.606       |\n",
      "|    win_rate           | 0.343       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.26       |\n",
      "| time/                 |             |\n",
      "|    fps                | 377         |\n",
      "|    iterations         | 2100        |\n",
      "|    time_elapsed       | 27          |\n",
      "|    total_timesteps    | 10500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000478   |\n",
      "|    explained_variance | -0.04134202 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 2099        |\n",
      "|    policy_loss        | 1.69e-05    |\n",
      "|    value_loss         | 0.821       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.208     |\n",
      "|    loss_rate          | 0.576      |\n",
      "|    win_rate           | 0.368      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.34      |\n",
      "| time/                 |            |\n",
      "|    fps                | 376        |\n",
      "|    iterations         | 2200       |\n",
      "|    time_elapsed       | 29         |\n",
      "|    total_timesteps    | 11000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000481  |\n",
      "|    explained_variance | -0.2931143 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2199       |\n",
      "|    policy_loss        | 9.7e-07    |\n",
      "|    value_loss         | 0.828      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.187     |\n",
      "|    loss_rate          | 0.566      |\n",
      "|    win_rate           | 0.379      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.17      |\n",
      "| time/                 |            |\n",
      "|    fps                | 378        |\n",
      "|    iterations         | 2300       |\n",
      "|    time_elapsed       | 30         |\n",
      "|    total_timesteps    | 11500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000465  |\n",
      "|    explained_variance | 0.81430274 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2299       |\n",
      "|    policy_loss        | -1.41e-05  |\n",
      "|    value_loss         | 0.232      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.166     |\n",
      "|    loss_rate          | 0.562      |\n",
      "|    win_rate           | 0.396      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.22      |\n",
      "| time/                 |            |\n",
      "|    fps                | 378        |\n",
      "|    iterations         | 2400       |\n",
      "|    time_elapsed       | 31         |\n",
      "|    total_timesteps    | 12000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000476  |\n",
      "|    explained_variance | -0.9845307 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2399       |\n",
      "|    policy_loss        | 7.41e-06   |\n",
      "|    value_loss         | 1.62       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.17      |\n",
      "|    loss_rate          | 0.56       |\n",
      "|    win_rate           | 0.39       |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | 0.03       |\n",
      "| time/                 |            |\n",
      "|    fps                | 376        |\n",
      "|    iterations         | 2500       |\n",
      "|    time_elapsed       | 33         |\n",
      "|    total_timesteps    | 12500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.00044   |\n",
      "|    explained_variance | 0.70752627 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2499       |\n",
      "|    policy_loss        | -7.85e-06  |\n",
      "|    value_loss         | 0.318      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.189      |\n",
      "|    loss_rate          | 0.565       |\n",
      "|    win_rate           | 0.376       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.24       |\n",
      "| time/                 |             |\n",
      "|    fps                | 376         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 34          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000451   |\n",
      "|    explained_variance | -0.19491434 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | -1.17e-05   |\n",
      "|    value_loss         | 0.848       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.203    |\n",
      "|    loss_rate          | 0.577     |\n",
      "|    win_rate           | 0.374     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.18     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 36        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000428 |\n",
      "|    explained_variance | 0.5232169 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 1.28e-05  |\n",
      "|    value_loss         | 0.571     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.223     |\n",
      "|    loss_rate          | 0.586      |\n",
      "|    win_rate           | 0.363      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.14      |\n",
      "| time/                 |            |\n",
      "|    fps                | 371        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 37         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000444  |\n",
      "|    explained_variance | 0.19395286 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 1.78e-07   |\n",
      "|    value_loss         | 0.516      |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.194      |\n",
      "|    loss_rate          | 0.569       |\n",
      "|    win_rate           | 0.375       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.17       |\n",
      "| time/                 |             |\n",
      "|    fps                | 372         |\n",
      "|    iterations         | 2900        |\n",
      "|    time_elapsed       | 38          |\n",
      "|    total_timesteps    | 14500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000393   |\n",
      "|    explained_variance | 0.054474473 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 2899        |\n",
      "|    policy_loss        | -5.05e-06   |\n",
      "|    value_loss         | 0.929       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.19      |\n",
      "|    loss_rate          | 0.568      |\n",
      "|    win_rate           | 0.378      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.13      |\n",
      "| time/                 |            |\n",
      "|    fps                | 371        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 40         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000364  |\n",
      "|    explained_variance | 0.32473767 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | -1.12e-05  |\n",
      "|    value_loss         | 0.553      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.217     |\n",
      "|    loss_rate          | 0.584      |\n",
      "|    win_rate           | 0.367      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.17      |\n",
      "| time/                 |            |\n",
      "|    fps                | 373        |\n",
      "|    iterations         | 3100       |\n",
      "|    time_elapsed       | 41         |\n",
      "|    total_timesteps    | 15500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.00036   |\n",
      "|    explained_variance | -0.6911092 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 3099       |\n",
      "|    policy_loss        | 3.16e-06   |\n",
      "|    value_loss         | 1.09       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| rates/                |              |\n",
      "|    avg_reward         | -0.217       |\n",
      "|    loss_rate          | 0.584        |\n",
      "|    win_rate           | 0.367        |\n",
      "| rollout/              |              |\n",
      "|    ep_len_mean        | 1            |\n",
      "|    ep_rew_mean        | 0.05         |\n",
      "| time/                 |              |\n",
      "|    fps                | 374          |\n",
      "|    iterations         | 3200         |\n",
      "|    time_elapsed       | 42           |\n",
      "|    total_timesteps    | 16000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -0.000322    |\n",
      "|    explained_variance | -0.027798772 |\n",
      "|    learning_rate      | 0.001        |\n",
      "|    n_updates          | 3199         |\n",
      "|    policy_loss        | -8.23e-06    |\n",
      "|    value_loss         | 1.07         |\n",
      "----------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.19      |\n",
      "|    loss_rate          | 0.575      |\n",
      "|    win_rate           | 0.385      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.19      |\n",
      "| time/                 |            |\n",
      "|    fps                | 372        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 44         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000334  |\n",
      "|    explained_variance | 0.07692772 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | 1.72e-05   |\n",
      "|    value_loss         | 1.24       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.181      |\n",
      "|    loss_rate          | 0.57        |\n",
      "|    win_rate           | 0.389       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.11       |\n",
      "| time/                 |             |\n",
      "|    fps                | 372         |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 45          |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000339   |\n",
      "|    explained_variance | -0.47822022 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | -8.53e-06   |\n",
      "|    value_loss         | 1.04        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.205      |\n",
      "|    loss_rate          | 0.574       |\n",
      "|    win_rate           | 0.369       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.27       |\n",
      "| time/                 |             |\n",
      "|    fps                | 373         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 46          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000356   |\n",
      "|    explained_variance | 0.059826672 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | -2.55e-06   |\n",
      "|    value_loss         | 0.91        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.194     |\n",
      "|    loss_rate          | 0.57       |\n",
      "|    win_rate           | 0.376      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.18      |\n",
      "| time/                 |            |\n",
      "|    fps                | 372        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 48         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000323  |\n",
      "|    explained_variance | 0.31793052 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -4.55e-07  |\n",
      "|    value_loss         | 0.655      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.144    |\n",
      "|    loss_rate          | 0.548     |\n",
      "|    win_rate           | 0.404     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.11     |\n",
      "| time/                 |           |\n",
      "|    fps                | 370       |\n",
      "|    iterations         | 3700      |\n",
      "|    time_elapsed       | 49        |\n",
      "|    total_timesteps    | 18500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000287 |\n",
      "|    explained_variance | 0.3079852 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 3699      |\n",
      "|    policy_loss        | -2.38e-05 |\n",
      "|    value_loss         | 1.37      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.11      |\n",
      "|    loss_rate          | 0.532      |\n",
      "|    win_rate           | 0.422      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.27      |\n",
      "| time/                 |            |\n",
      "|    fps                | 370        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 51         |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000304  |\n",
      "|    explained_variance | 0.43258214 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | -2.39e-06  |\n",
      "|    value_loss         | 0.372      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.135     |\n",
      "|    loss_rate          | 0.549      |\n",
      "|    win_rate           | 0.414      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.14      |\n",
      "| time/                 |            |\n",
      "|    fps                | 370        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000301  |\n",
      "|    explained_variance | 0.23863482 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 5.58e-07   |\n",
      "|    value_loss         | 0.731      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.199    |\n",
      "|    loss_rate          | 0.58      |\n",
      "|    win_rate           | 0.381     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.27     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 4000      |\n",
      "|    time_elapsed       | 53        |\n",
      "|    total_timesteps    | 20000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000315 |\n",
      "|    explained_variance | nan       |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 3999      |\n",
      "|    policy_loss        | 2.49e-05  |\n",
      "|    value_loss         | 1.32      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.189     |\n",
      "|    loss_rate          | 0.571      |\n",
      "|    win_rate           | 0.382      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.16      |\n",
      "| time/                 |            |\n",
      "|    fps                | 371        |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 55         |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000288  |\n",
      "|    explained_variance | 0.84393716 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | -4.42e-06  |\n",
      "|    value_loss         | 0.13       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.186      |\n",
      "|    loss_rate          | 0.572       |\n",
      "|    win_rate           | 0.386       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.12       |\n",
      "| time/                 |             |\n",
      "|    fps                | 371         |\n",
      "|    iterations         | 4200        |\n",
      "|    time_elapsed       | 56          |\n",
      "|    total_timesteps    | 21000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000269   |\n",
      "|    explained_variance | -0.06472194 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4199        |\n",
      "|    policy_loss        | -9.12e-06   |\n",
      "|    value_loss         | 0.834       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.198      |\n",
      "|    loss_rate          | 0.578       |\n",
      "|    win_rate           | 0.38        |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.31       |\n",
      "| time/                 |             |\n",
      "|    fps                | 372         |\n",
      "|    iterations         | 4300        |\n",
      "|    time_elapsed       | 57          |\n",
      "|    total_timesteps    | 21500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000294   |\n",
      "|    explained_variance | -0.04087782 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4299        |\n",
      "|    policy_loss        | 2.38e-05    |\n",
      "|    value_loss         | 1.54        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.175      |\n",
      "|    loss_rate          | 0.561       |\n",
      "|    win_rate           | 0.386       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.27       |\n",
      "| time/                 |             |\n",
      "|    fps                | 371         |\n",
      "|    iterations         | 4400        |\n",
      "|    time_elapsed       | 59          |\n",
      "|    total_timesteps    | 22000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000269   |\n",
      "|    explained_variance | -0.07322073 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4399        |\n",
      "|    policy_loss        | 1.09e-05    |\n",
      "|    value_loss         | 0.907       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.187      |\n",
      "|    loss_rate          | 0.566       |\n",
      "|    win_rate           | 0.379       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.26       |\n",
      "| time/                 |             |\n",
      "|    fps                | 371         |\n",
      "|    iterations         | 4500        |\n",
      "|    time_elapsed       | 60          |\n",
      "|    total_timesteps    | 22500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000259   |\n",
      "|    explained_variance | -0.20471871 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4499        |\n",
      "|    policy_loss        | -1.83e-06   |\n",
      "|    value_loss         | 0.971       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.194      |\n",
      "|    loss_rate          | 0.57        |\n",
      "|    win_rate           | 0.376       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.19       |\n",
      "| time/                 |             |\n",
      "|    fps                | 371         |\n",
      "|    iterations         | 4600        |\n",
      "|    time_elapsed       | 61          |\n",
      "|    total_timesteps    | 23000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.000269   |\n",
      "|    explained_variance | -0.17063284 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4599        |\n",
      "|    policy_loss        | 2.6e-06     |\n",
      "|    value_loss         | 0.761       |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rates/                |             |\n",
      "|    avg_reward         | -0.211      |\n",
      "|    loss_rate          | 0.576       |\n",
      "|    win_rate           | 0.365       |\n",
      "| rollout/              |             |\n",
      "|    ep_len_mean        | 1           |\n",
      "|    ep_rew_mean        | -0.09       |\n",
      "| time/                 |             |\n",
      "|    fps                | 371         |\n",
      "|    iterations         | 4700        |\n",
      "|    time_elapsed       | 63          |\n",
      "|    total_timesteps    | 23500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -0.00025    |\n",
      "|    explained_variance | -0.13791323 |\n",
      "|    learning_rate      | 0.001       |\n",
      "|    n_updates          | 4699        |\n",
      "|    policy_loss        | 1.4e-05     |\n",
      "|    value_loss         | 1.53        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rates/                |            |\n",
      "|    avg_reward         | -0.235     |\n",
      "|    loss_rate          | 0.588      |\n",
      "|    win_rate           | 0.353      |\n",
      "| rollout/              |            |\n",
      "|    ep_len_mean        | 1          |\n",
      "|    ep_rew_mean        | -0.19      |\n",
      "| time/                 |            |\n",
      "|    fps                | 371        |\n",
      "|    iterations         | 4800       |\n",
      "|    time_elapsed       | 64         |\n",
      "|    total_timesteps    | 24000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -0.000237  |\n",
      "|    explained_variance | 0.70332897 |\n",
      "|    learning_rate      | 0.001      |\n",
      "|    n_updates          | 4799       |\n",
      "|    policy_loss        | -3.05e-06  |\n",
      "|    value_loss         | 0.306      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.254    |\n",
      "|    loss_rate          | 0.599     |\n",
      "|    win_rate           | 0.345     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.23     |\n",
      "| time/                 |           |\n",
      "|    fps                | 371       |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 65        |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000239 |\n",
      "|    explained_variance | 0.7588688 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | -7.52e-08 |\n",
      "|    value_loss         | 0.231     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| rates/                |           |\n",
      "|    avg_reward         | -0.189    |\n",
      "|    loss_rate          | 0.568     |\n",
      "|    win_rate           | 0.379     |\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 1         |\n",
      "|    ep_rew_mean        | -0.04     |\n",
      "| time/                 |           |\n",
      "|    fps                | 372       |\n",
      "|    iterations         | 5000      |\n",
      "|    time_elapsed       | 67        |\n",
      "|    total_timesteps    | 25000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -0.000226 |\n",
      "|    explained_variance | 0.6434432 |\n",
      "|    learning_rate      | 0.001     |\n",
      "|    n_updates          | 4999      |\n",
      "|    policy_loss        | -3.91e-06 |\n",
      "|    value_loss         | 0.27      |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 7\u001B[0m\n\u001B[0;32m      5\u001B[0m env\u001B[38;5;241m.\u001B[39mreset()\n\u001B[0;32m      6\u001B[0m model \u001B[38;5;241m=\u001B[39m A2C(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, tensorboard_log\u001B[38;5;241m=\u001B[39mlogdir, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.001\u001B[39m)\n\u001B[1;32m----> 7\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m500000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mTensorboardCallback\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mA2C_no_memory\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001B[0m, in \u001B[0;36mA2C.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    192\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    193\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfA2C,\n\u001B[0;32m    194\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    199\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    200\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfA2C:\n\u001B[1;32m--> 201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    202\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtotal_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtotal_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    203\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallback\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    204\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlog_interval\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    205\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtb_log_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtb_log_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    206\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset_num_timesteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    207\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    208\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    297\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    299\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 300\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect_rollouts\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menv\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrollout_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_rollout_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_steps\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    302\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    303\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:179\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    176\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m th\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m    177\u001B[0m     \u001B[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001B[39;00m\n\u001B[0;32m    178\u001B[0m     obs_tensor \u001B[38;5;241m=\u001B[39m obs_as_tensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_obs, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m--> 179\u001B[0m     actions, values, log_probs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpolicy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mobs_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m actions \u001B[38;5;241m=\u001B[39m actions\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[0;32m    182\u001B[0m \u001B[38;5;66;03m# Rescale and perform action\u001B[39;00m\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1734\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1735\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1736\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1742\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1743\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1744\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1745\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1746\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1747\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1749\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\workspace\\.virtualenvs\\decision_under_uncertainty\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:657\u001B[0m, in \u001B[0;36mActorCriticPolicy.forward\u001B[1;34m(self, obs, deterministic)\u001B[0m\n\u001B[0;32m    655\u001B[0m actions \u001B[38;5;241m=\u001B[39m distribution\u001B[38;5;241m.\u001B[39mget_actions(deterministic\u001B[38;5;241m=\u001B[39mdeterministic)\n\u001B[0;32m    656\u001B[0m log_prob \u001B[38;5;241m=\u001B[39m distribution\u001B[38;5;241m.\u001B[39mlog_prob(actions)\n\u001B[1;32m--> 657\u001B[0m actions \u001B[38;5;241m=\u001B[39m \u001B[43mactions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreshape\u001B[49m\u001B[43m(\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43maction_space\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m    658\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m actions, values, log_prob\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for episode in range(10):\n",
    "    print(\"------------------------------------------------------\")\n",
    "    state = env.reset()[0]\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "    \n",
    "    \n",
    "    env.render()\n",
    "    while not done:\n",
    "        action, _ = model.predict(state, deterministic=True)\n",
    "        state, reward, done, truncated, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        env.render()\n",
    "    \n",
    "    if episode_reward == 1:\n",
    "        print(f\"Player won this round.\")\n",
    "    if episode_reward == -1:\n",
    "        print(f\"Player lost this round.\")\n",
    "    if episode_reward == 0:\n",
    "        print(f\"This round was a tie.\")"
   ],
   "id": "e939ef1dd42419d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "2c8aab1940815144",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
